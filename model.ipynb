{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOec8RtPCgpP2MMwxGa0Wz9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from __future__ import print_function\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim\n","from torch.utils import data\n","# from torchvision import datasets, transforms\n","from torch.autograd import Variable\n","import time\n","import numpy as np\n","import os\n","from copy import deepcopy\n","from itertools import chain\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UqN3SLkZU8VB","executionInfo":{"status":"ok","timestamp":1675027892367,"user_tz":300,"elapsed":31931,"user":{"displayName":"Jie Sun","userId":"06086794844872432576"}},"outputId":"1bc4924b-7dcc-40c4-b6d4-2fb0a75d4b32"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"UNZqItuv2Ckz","executionInfo":{"status":"ok","timestamp":1675028854387,"user_tz":300,"elapsed":139,"user":{"displayName":"Jie Sun","userId":"06086794844872432576"}}},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self, num_classes, hidden_size, num_layers, middle_feature):\n","        super(Net, self).__init__()\n","        self.num_classes = num_classes\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.bidirectional = False\n","        self.middle_feature = middle_feature\n","\n","        self.embedding = nn.Sequential(\n","            nn.Conv2d(1, 2, kernel_size=(3, 1), padding=(1, 0), stride=1),\n","            nn.ReLU(),\n","            nn.Conv2d(2, 4, kernel_size=(3, 1), padding=(1, 0), stride=1),\n","            nn.ReLU())\n","\n","        self.embedding_dim = 129 * 4\n","\n","        self.BiLSTM = nn.LSTM(input_size=self.embedding_dim,\n","                              hidden_size=self.hidden_size,\n","                              num_layers=self.num_layers,\n","                              dropout=0.2,\n","                              bidirectional=self.bidirectional)\n","\n","        self.MLP = nn.Sequential(\n","            nn.Linear(self.hidden_size, self.middle_feature),\n","            nn.ReLU(),\n","            nn.Linear(self.middle_feature, num_classes)\n","        )\n","\n","    def forward(self, datas):\n","        batch, t, f = datas.shape\n","        print('input:', batch, f, t)\n","        datas = datas.view(batch, 1, f, t)\n","        print('reshape:', datas.size())\n","        embedded_datas = self.embedding(datas)\n","        print('after conv:', embedded_datas.size())\n","        embedded_datas = embedded_datas.permute(3, 0, 1, 2).view(t, batch, -1)\n","        print('before lstm:', embedded_datas.size())\n","        out, _ = self.BiLSTM(embedded_datas)\n","        # print('after lstm:',out.size())\n","        # out, _ =pad_packed_sequence(out)\n","        # out = out[-1,:,:]\n","        print('reshape:', out.size())\n","        out = self.MLP(out)\n","        return out\n","\n","\n","class Netone(nn.Module):\n","    def __init__(self, num_classes, hidden_size, num_layers, middle_feature):\n","        super(Netone, self).__init__()\n","        self.num_classes = num_classes\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.bidirectional = False\n","        self.middle_feature = middle_feature\n","        \n","        self.embedding = nn.Sequential(\n","            nn.Conv2d(1, 2, kernel_size=(3, 1), padding=(1, 0), stride=1),\n","            nn.ReLU(),\n","            nn.Conv2d(2, 4, kernel_size=(3, 1), padding=(1, 0), stride=1),\n","            nn.ReLU())\n","\n","        self.embedding_dim = 129\n","\n","        self.BiLSTM = nn.LSTM(input_size=self.embedding_dim,\n","                              hidden_size=self.hidden_size,\n","                              num_layers=self.num_layers,\n","                              dropout=0.2,\n","                              bidirectional=self.bidirectional,\n","                              batch_first=True)\n","\n","        self.MLP = nn.Sequential(\n","            nn.Linear(self.hidden_size, self.middle_feature),\n","            nn.ReLU(),\n","            nn.Linear(self.middle_feature, num_classes)\n","        )  # remove the mlp and generate the irritation state at each time step\n","\n","# only LSTM\n","    def forward(self, datas):\n","        # print('Before LSTM:{}'.format(datas.size()))\n","        out, _ = self.BiLSTM(datas)\n","        # print('After LSTM:{}'.format(out.size()))\n","        # out = out[-1, :, :]  # 20,512\n","        out = self.MLP(out)\n","        out = torch.squeeze(out)\n","        # print('output shape:{}'.format(out.shape))\n","        return out\n","\n","# # CNN + LSTM\n","#     def forward(self, datas):\n","#         t, f = datas.shape\n","#         embedded_datas = self.embedding(datas)\n","#         print('After conv:{}\\n'.format(embedded_datas.size())) # [20, 4, 54, 129]\n","#         embedded_datas = embedded_datas.permute(3, 0, 1, 2).view(f, 1, -1)\n","#         print('Before LSTM:{}'.format(datas.size()))\n","#         out, _ = self.BiLSTM(embedded_datas)\n","#         print('After LSTM:{}'.format(out.size()))\n","#         # out = out[-1, :, :]  # 20,512\n","#         out = self.MLP(out)\n","#         out = torch.squeeze(out)\n","#         # print('output shape:{}'.format(out.shape))\n","#         return out"]}]}